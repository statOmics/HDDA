<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Adapted by Milan Malfait and Leo Fuhrhop" />


<title>Lab 3: Penalized regression techniques for high-dimensional data</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script src="site_libs/navigation-1.1/sourceembed.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-6.5.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #204a87; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #204a87; font-weight: bold; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */

.sourceCode .row {
  width: 100%;
}
.sourceCode {
  overflow-x: auto;
}
.code-folding-btn {
  margin-right: -30px;
}
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>


<style type="text/css">
#rmd-source-code {
  display: none;
}
</style>


<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">HDDA</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-chalkboard-teacher"></span>
     
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="intro.html">1. Introduction</a>
    </li>
    <li>
      <a href="svd.html">2. Singular Value Decomposition</a>
    </li>
    <li>
      <a href="prediction.html">3. Prediction with High Dimensional Predictors</a>
    </li>
    <li>
      <a href="sparseSvd.html">4. Sparse Singular Value Decomposition</a>
    </li>
    <li>
      <a href="lda.html">5. Linear Discriminant Analysis</a>
    </li>
    <li>
      <a href="hclust.html">6.1. Introduction to Clustering</a>
    </li>
    <li>
      <a href="https://sites.stat.washington.edu/people/raftery/Research/PDF/fraley1998.pdf">6.2. Paper Model-based Clustering</a>
    </li>
    <li>
      <a href="em.html">6.3. EM algorithm</a>
    </li>
    <li>
      <a href="lsi.html">7. Large Scale Inference</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-laptop"></span>
     
    Labs
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Lab1-Intro-SVD.html">Lab 1: Intro &amp; SVD</a>
    </li>
    <li>
      <a href="Lab2-PCA.html">Lab 2: SVD - PCA</a>
    </li>
    <li>
      <a href="Lab3-Penalized-Regression.html">Lab 3: Prediction</a>
    </li>
    <li>
      <a href="Lab4-Sparse-PCA-LDA.html">Lab 4: Sparse PCA &amp; LDA</a>
    </li>
    <li>
      <a href="Lab5-Clustering.html">Lab 5: Clustering</a>
    </li>
    <li>
      <a href="Lab6-Large-Scale-Inference.html">Lab 6: LSI</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="https://github.com/statOmics/HDDA">
    <span class="fab fa-github"></span>
     
  </a>
</li>
<li>
  <a href="http://statomics.github.io/">statOmics</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
<li role="separator" class="divider"></li>
<li><a id="rmd-download-source" href="#">Download Rmd</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Lab 3: Penalized regression techniques for high-dimensional data</h1>
<h3 class="subtitle">High Dimensional Data Analysis practicals</h3>
<h4 class="author">Adapted by Milan Malfait and Leo Fuhrhop</h4>
<h4 class="date">04 Nov 2021 <br/> (Last updated: 2025-11-06)</h4>

</div>


<div id="change-log" class="section level3 unnumbered">
<h3><a href="https://github.com/statOmics/HDDA/commits/master/Lab3-Penalized-Regression.Rmd">Change log</a></h3>
<hr />
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="do">## install packages with:</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co"># install.packages(c(&quot;glmnet&quot;, &quot;pls&quot;, &quot;boot&quot;))</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># remotes::install_github(&quot;statOmics/HDDAData&quot;)</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="fu">library</span>(HDDAData)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="fu">library</span>(pls)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="fu">library</span>(boot)</span></code></pre></div>
</div>
<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p><strong>In this lab session we will look at the following topics</strong></p>
<ul>
<li>Demonstrate why low-dimensional prediction modeling fails in high-dimensional settings.</li>
<li>Carry out Principal Component Regression (PCR).</li>
<li>Use <code>glmnet()</code> to carry out ridge regression, lasso and elastic net.</li>
<li>Evaluate these trained prediction models.</li>
</ul>
<div id="the-dataset" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> The dataset</h2>
<p>In this practical, we will use the dataset <code>eyedata</code> provided by
the <a href="https://cran.r-project.org/web/packages/NormalBetaPrime/index.html"><strong>NormalBetaPrime</strong> package</a>.
This dataset contains gene expression data of 200
genes for 120 samples. The data originates from microarray experiments
of mammalian eye tissue samples.</p>
<p>The dataset consists of two objects:</p>
<ul>
<li><code>genes</code>: a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>120</mn><mo>Ã—</mo><mn>200</mn></mrow><annotation encoding="application/x-tex">120 \times 200</annotation></semantics></math> matrix with the expression levels of 200 genes
(columns) for 120 samples (rows)</li>
<li><code>trim32</code>: a vector with 120 expression levels of the TRIM32 gene.</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="fu">data</span>(eyedata)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>genes <span class="ot">&lt;-</span> eyedata<span class="sc">$</span>genes</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>trim32 <span class="ot">&lt;-</span> eyedata<span class="sc">$</span>trim32</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="do">## Look at objects that were just loaded</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="fu">str</span>(genes)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co">#&gt;  num [1:120, 1:200] 3.68 3.58 3.85 4.13 3.88 ...</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co">#&gt;  - attr(*, &quot;dimnames&quot;)=List of 2</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="co">#&gt;   ..$ : chr [1:120] &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; &quot;V5&quot; ...</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co">#&gt;   ..$ : chr [1:200] &quot;1377&quot; &quot;1748&quot; &quot;2487&quot; &quot;2679&quot; ...</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="fu">str</span>(trim32)</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co">#&gt;  num [1:120] 8.42 8.36 8.41 8.29 8.27 ...</span></span></code></pre></div>
<p>The goal of this exercise is to predict the expression levels of
TRIM32 from the expression levels of the 200 genes measured in the
microarray experiment. For this, it makes sense to start by constructing
centered (and possibly scaled) data. We store this in two matrices
<code>X</code> and <code>Y</code>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">scale</span>(genes, <span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">scale</span>(trim32, <span class="at">center =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Remember that scaling avoids that differences in levels of magnitude
will give one variable (gene) more influence in the result. This has
been illustrated in the <a href="./Lab2-PCA.html">second practical session</a> as well.
For the <code>Y</code> vector, this is less of an issue as weâ€™re talking about a single variable.
Not scaling will make the predictions interpretable as â€œdeviations from the
meanâ€.</p>
</div>
<div id="the-curse-of-singularity" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> The curse of singularity</h2>
<p>We begin by assuming that the predictors and the outcome have been
centered so that the intercept is 0.
We are presented with the usual regression model:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>Î²</mi><mi>i</mi></msub><msub><mi>X</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>+</mo><mi>â€¦</mi><mo>+</mo><msub><mi>Î²</mi><mi>p</mi></msub><msub><mi>X</mi><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo>+</mo><msub><mi>Ïµ</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> Or </mtext><mspace width="0.333em"></mspace></mrow><mi>ğ˜</mi><mo>=</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><mo>+</mo><mi>ğ›œ</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
&amp;Y_i=\beta_i X_{i1}+\dots+\beta_pX_{ip}+\epsilon_i \\
&amp;\text{ Or } \mathbf{Y}={\mathbf{X}}{\boldsymbol{\beta}} +{\boldsymbol{\epsilon}}
\end{align*}</annotation></semantics></math></p>
<p>Our goal is to get the least squares estimator of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ›ƒ</mi><annotation encoding="application/x-tex">{\boldsymbol{\beta}}</annotation></semantics></math>, given by</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>ğ—</mi><mi>T</mi></msup><mi>ğ—</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><msup><mi>ğ—</mi><mi>T</mi></msup><mi>ğ˜</mi></mrow><annotation encoding="application/x-tex">
\hat{{\boldsymbol{\beta}}}= (\mathbf{X}^T{\mathbf{X}})^{-1}{\mathbf{X}}^T{\mathbf{Y}}
</annotation></semantics></math></p>
<p>in which the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>Ã—</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">p \times p</annotation></semantics></math> matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>ğ—</mi><mi>T</mi></msup><mi>ğ—</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">({\mathbf{X}}^T{\mathbf{X}})^{-1}</annotation></semantics></math> is crucial!
To be able to calculate the inverse of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ğ—</mi><mi>T</mi></msup><mi>ğ—</mi></mrow><annotation encoding="application/x-tex">{\mathbf{X}}^T \mathbf{X}</annotation></semantics></math>,
it has to be of full rank <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, which would be 200 in this case.
Letâ€™s check this:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="fu">dim</span>(X) <span class="co"># 120 x 200, so p &gt; n!</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="co">#&gt; [1] 120 200</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="fu">qr</span>(X)<span class="sc">$</span>rank</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co">#&gt; [1] 119</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>XtX <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(X) <span class="co"># calculates t(X) %*% X more efficiently</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="fu">qr</span>(XtX)<span class="sc">$</span>rank</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="co">#&gt; [1] 119</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="co"># Try to invert using solve:</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="fu">solve</span>(XtX)</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="co">#&gt; Error in solve.default(XtX): system is computationally singular: reciprocal condition number = 4.54594e-20</span></span></code></pre></div>
<p>We realize we cannot compute
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>ğ—</mi><mi>T</mi></msup><mi>ğ—</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">({\mathbf{X}}^T{\mathbf{X}})^{-1}</annotation></semantics></math> because the rank of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>ğ—</mi><mi>T</mi></msup><mi>ğ—</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">({\mathbf{X}}^T{\mathbf{X}})</annotation></semantics></math> is less than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> hence we canâ€™t
get <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><annotation encoding="application/x-tex">\hat{{\boldsymbol{\beta}}}</annotation></semantics></math> by means of least squares!
This is generally referred to as the <strong><a href="https://www.statistics.com/glossary/singularity/">singularity</a> problem</strong>.</p>
</div>
</div>
<div id="principal-component-regression" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Principal component regression</h1>
<p>A first way to deal with this singularity, is to bypass it using principal components.
Since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>n</mi><mo>=</mo><mn>120</mn></mrow><annotation encoding="application/x-tex">\min(n,p) = n = 120</annotation></semantics></math>,
PCA will give 120 components, each being a linear combination of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> = 200 variables.
These 120 PCs contain all information present in the original data.
We could as well use an approximation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ—</mi><annotation encoding="application/x-tex">{\mathbf{X}}</annotation></semantics></math>, i.e using just a few (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&lt;</mo><mn>120</mn></mrow><annotation encoding="application/x-tex">k&lt;120</annotation></semantics></math>) PCs.
So we use PCA as a method for reducing the dimensions while retaining
as much variation between the observations as possible.
Once we have these PCs, we can use them as variables in a linear regression model.</p>
<div id="classic-linear-regression-on-pcs" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Classic linear regression on PCs</h2>
<p>We first compute the PCA on the data with <code>prcomp</code>.
We will use an arbitrary cutoff of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">k = 4</annotation></semantics></math> PCs to illustrate the process of performing regression on the PCs.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">4</span> <span class="co"># Arbitrarily chosen k=4</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>pca <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(X)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>Vk <span class="ot">&lt;-</span> pca<span class="sc">$</span>rotation[, <span class="dv">1</span><span class="sc">:</span>k] <span class="co"># the loadings matrix</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>Zk <span class="ot">&lt;-</span> pca<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span>k] <span class="co"># the scores matrix</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="co"># Use the scores in classic linear regression</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>pcr_model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> Zk)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="fu">summary</span>(pcr_model1)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="co">#&gt; lm(formula = Y ~ Zk)</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a><span class="co">#&gt;      Min       1Q   Median       3Q      Max </span></span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a><span class="co">#&gt; -1.72388 -0.34723  0.02811  0.27817  2.03271 </span></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a><span class="co">#&gt; (Intercept) -1.494e-14  5.454e-02   0.000   1.0000    </span></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a><span class="co">#&gt; ZkPC1       -7.172e-02  4.950e-03 -14.488   &lt;2e-16 ***</span></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a><span class="co">#&gt; ZkPC2        1.273e-02  1.342e-02   0.949   0.3447    </span></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a><span class="co">#&gt; ZkPC3        3.371e-02  2.326e-02   1.449   0.1500    </span></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a><span class="co">#&gt; ZkPC4        5.908e-02  2.535e-02   2.330   0.0215 *  </span></span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 0.5975 on 115 degrees of freedom</span></span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.655,  Adjusted R-squared:  0.643 </span></span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a><span class="co">#&gt; F-statistic: 54.58 on 4 and 115 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>As <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ—</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ˜</mi><annotation encoding="application/x-tex">\mathbf{Y}</annotation></semantics></math> are centered, the intercept is
approximately 0.</p>
<p>The output shows that PC1 and PC4 have a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î²</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> estimate that
differs significantly from 0 (at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow><annotation encoding="application/x-tex">p &lt; 0.05</annotation></semantics></math>), but the results canâ€™t be readily
interpreted, since we have no immediate interpretation of the PCs.</p>
</div>
<div id="using-the-package-pls" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Using the package <code>pls</code></h2>
<p>PCR can also be performed using the <code>pcr()</code> function from the
package <em><a href="https://CRAN.R-project.org/package=pls">pls</a></em>
<strong>directly on the data</strong> (so without having to first perform the PCA manually).
When using this function, you have to keep a few things in mind:</p>
<ol style="list-style-type: decimal">
<li>the number of components (PCs) to use is passed with the argument <code>ncomp</code></li>
<li>the function allows you to scale (set <code>scale = TRUE</code>) and
center (set <code>center = TRUE</code>) the predictors first (in the example here, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ—</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> has already been centered and scaled).</li>
</ol>
<p>You can use the function <code>pcr()</code> in much the same way as you would
use <code>lm()</code>. The resulting fit can easily be examined using the
function <code>summary()</code>, but the output looks quite different from
what you would get from <code>lm</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># X is already scaled and centered, so that&#39;s not needed.</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>pcr_model2 <span class="ot">&lt;-</span> <span class="fu">pcr</span>(Y <span class="sc">~</span> X, <span class="at">ncomp =</span> <span class="dv">4</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="fu">summary</span>(pcr_model2)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co">#&gt; Data:    X dimension: 120 200 </span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="co">#&gt;  Y dimension: 120 1</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a><span class="co">#&gt; Fit method: svdpc</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a><span class="co">#&gt; Number of components considered: 4</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a><span class="co">#&gt; TRAINING: % variance explained</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a><span class="co">#&gt;    1 comps  2 comps  3 comps  4 comps</span></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="co">#&gt; X    61.22    69.55    72.33    74.66</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a><span class="co">#&gt; Y    62.97    63.24    63.87    65.50</span></span></code></pre></div>
<p>First of all the output shows you the data dimensions and the fitting
method used. In this case, that is PC calculation based on SVD. The
<code>summary()</code> function also provides the percentage of variance
explained in the predictors and in the response using different numbers
of components. For example, the first PC only captures 61.22% of all
the variance, or information in the predictors and it explains 62.9%
of the variance in the outcome. Note that for both methods the choice of
the number of principal components was arbitrary chosen to be 4.</p>
<p>At a later stage, we will look at how to choose the number of components
that has the <strong>smallest prediction error</strong>.</p>
</div>
</div>
<div id="elnet-theory" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Ridges, Lassos and Elastic Nets</h1>
<p>Ridge regression, lasso regression and elastic nets are all closely
related techniques, based on the same idea: add a penalty term to
the estimating function so <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>ğ—</mi><mi>T</mi></msup><mi>ğ—</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">({\mathbf{X}}^T{\mathbf{X}})</annotation></semantics></math>
becomes full rank again and is invertible. Two different penalty
terms or regularization methods can be used:</p>
<ol style="list-style-type: decimal">
<li>L1 regularization: this regularization adds a term <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mo stretchy="false" form="postfix">âˆ¥</mo><mi>ğ›ƒ</mi><msub><mo stretchy="false" form="postfix">âˆ¥</mo><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">{\lambda\|\boldsymbol{\beta}\|_{1}}</annotation></semantics></math> to the least squares criterion.
The term will add a penalty based on the <em>absolute value</em> of the
magnitude of the coefficients. This is used by <strong>lasso regression</strong>.</li>
</ol>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><mtext mathvariant="normal">lasso</mtext></msup><mo>=</mo><msub><mtext mathvariant="normal">argmin</mtext><mi>ğ›ƒ</mi></msub><mo stretchy="false" form="prefix">(</mo><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ˜</mi><mo>âˆ’</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ˜</mi><mo>âˆ’</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi>Î»</mi><mo stretchy="false" form="postfix">âˆ¥</mo><mi>ğ›ƒ</mi><msub><mo stretchy="false" form="postfix">âˆ¥</mo><mn>1</mn></msub></mrow></mrow><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
 \hat{\boldsymbol{\beta}}^{\text{lasso}} = \text{argmin}_{\boldsymbol{\beta}}\displaystyle({(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^T(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+{\lambda\|\boldsymbol{\beta}\|_{1}}}\displaystyle)
</annotation></semantics></math></p>
<ol start="2" style="list-style-type: decimal">
<li>L2 regularization: this regularization adds a term <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mo stretchy="false" form="postfix">âˆ¥</mo><mi>ğ›ƒ</mi><msubsup><mo stretchy="false" form="postfix">âˆ¥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">{\lambda\|\boldsymbol{\beta}\|_{2}^{2}}</annotation></semantics></math> to the least squares criterion.
The penalty term is based on the square of the magnitude of the
coefficients. This is used by <strong>ridge regression</strong>.</li>
</ol>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><mtext mathvariant="normal">ridge</mtext></msup><mo>=</mo><msub><mtext mathvariant="normal">argmin</mtext><mi>ğ›ƒ</mi></msub><mo stretchy="false" form="prefix">(</mo><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ˜</mi><mo>âˆ’</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ˜</mi><mo>âˆ’</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi>Î»</mi><mo stretchy="false" form="postfix">âˆ¥</mo><mi>ğ›ƒ</mi><msubsup><mo stretchy="false" form="postfix">âˆ¥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
 \hat{\boldsymbol{\beta}}^{\text{ridge}} = \text{argmin}_{\boldsymbol{\beta}}\displaystyle({(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^T(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+{\lambda\|\boldsymbol{\beta}\|_{2}^{2}}}\displaystyle)
</annotation></semantics></math></p>
<p>Elastic net regression combines both types of regularization. It does so by
introducing a mixing parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î±</mi><mo>âˆˆ</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha \in [0, 1]</annotation></semantics></math> that essentially combines
the L1 and L2 norms in a weighted average.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><mtext mathvariant="normal">el.net</mtext></msup><mo>=</mo><msub><mtext mathvariant="normal">argmin</mtext><mi>ğ›ƒ</mi></msub><mo stretchy="false" form="prefix">(</mo><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ˜</mi><mo>âˆ’</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ˜</mi><mo>âˆ’</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mi>Î±</mi><mi>Î»</mi><mo stretchy="false" form="postfix">âˆ¥</mo><mi>ğ›ƒ</mi><msub><mo stretchy="false" form="postfix">âˆ¥</mo><mn>1</mn></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Î±</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>Î»</mi><mo stretchy="false" form="postfix">âˆ¥</mo><mi>ğ›ƒ</mi><msubsup><mo stretchy="false" form="postfix">âˆ¥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
 \hat{\boldsymbol{\beta}}^{\text{el.net}} = \text{argmin}_{\boldsymbol{\beta}}\displaystyle({(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^{T}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+{\alpha \lambda\|\boldsymbol{\beta}\|_{1}}+ {(1 - \alpha)\lambda\|\boldsymbol{\beta}\|_{2}^{2}}}\displaystyle)
</annotation></semantics></math></p>
</div>
<div id="exercise-verification-of-ridge-regression" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Exercise: Verification of ridge regression</h1>
<p>In least square regression the minimization of the estimation function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><mrow><mi>ğ˜</mi><mo>âˆ’</mo><mi>ğ—</mi><mi>ğ›ƒ</mi></mrow><msubsup><mo stretchy="false" form="postfix">âˆ¥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">|{\mathbf{Y} - \mathbf{X} \boldsymbol{\beta}}\|^{2}_{2}</annotation></semantics></math> leads to the solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><msup><mi>ğ—</mi><mi>ğ“</mi></msup><mi>ğ—</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><mrow><msup><mi>ğ—</mi><mi>ğ“</mi></msup><mi>ğ˜</mi></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\hat{\beta}}=(\mathbf{X^TX})^{-1}\mathbf{X^TY}}</annotation></semantics></math>.</p>
<p>For the penalized least squares criterion used by ridge regression, you minimize
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">âˆ¥</mo><mrow><mi>ğ˜</mi><mo>âˆ’</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><msubsup><mo stretchy="false" form="postfix">âˆ¥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mi>Î»</mi><mrow><mo stretchy="false" form="postfix" mathvariant="bold">âˆ¥</mo><mi>ğ›ƒ</mi><msubsup><mo stretchy="false" form="postfix" mathvariant="bold">âˆ¥</mo><mn>ğŸ</mn><mn>ğŸ</mn></msubsup></mrow></mrow><annotation encoding="application/x-tex">\|{\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}\|^{2}_{2}}+\lambda{\boldsymbol{\|\beta\|^{2}_{2}}}</annotation></semantics></math>
which leads to following solution:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mrow><msup><mi>ğ—</mi><mi>ğ“</mi></msup><mi>ğ—</mi></mrow></mrow><mo>+</mo><mi>Î»</mi><mi>ğˆ</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><mrow><msup><mi>ğ—</mi><mi>ğ“</mi></msup><mi>ğ˜</mi></mrow></mrow><annotation encoding="application/x-tex">
{\boldsymbol{\hat{\beta}}=(\mathbf{X^TX}}+\lambda{\mathbf{I}})^{-1}{\mathbf{X^TY}}
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğˆ</mi><annotation encoding="application/x-tex">\mathbf{I}</annotation></semantics></math> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>Ã—</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">p \times p</annotation></semantics></math> identity matrix.</p>
<p>The ridge parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> <em>shrinks</em> the coefficients towards 0, with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda = 0</annotation></semantics></math> being equivalent to OLS (no shrinkage) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mo>=</mo><mi>+</mi><mi>âˆ</mi></mrow><annotation encoding="application/x-tex">\lambda = +\infty</annotation></semantics></math> being equivalent to setting all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>Î²</mi><mo accent="true">Ì‚</mo></mover><annotation encoding="application/x-tex">\hat{\beta}</annotation></semantics></math>â€™s to 0.
The optimal parameter lies somewhere in between and needs to be tuned by the user.</p>
<div id="tasks" class="section level2 unnumbered">
<h2>Tasks</h2>
<p>Solve the following exercises using R.</p>
<div id="verify-that-mathbfxtxlambdamathbfi-has-rank-200-for-any-lambda0-of-your-choice." class="section level4 unnumbered">
<h4>1. Verify that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="false" form="prefix" mathvariant="bold">(</mo><msup><mi>ğ—</mi><mi>ğ“</mi></msup><mi>ğ—</mi></mrow><mo>+</mo><mi>Î»</mi><mi>ğˆ</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">{\mathbf{(X^TX}}+\lambda{\mathbf{I}})</annotation></semantics></math> has rank <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>200</mn><annotation encoding="application/x-tex">200</annotation></semantics></math>, for any <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda&gt;0</annotation></semantics></math> of your choice.</h4>
<details>
<summary>
Solution
</summary>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>XtX <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(X)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co"># My choice</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co"># Compute penalized matrix</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>XtX_lambdaI <span class="ot">&lt;-</span> XtX <span class="sc">+</span> (lambda <span class="sc">*</span> <span class="fu">diag</span>(p))</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="fu">dim</span>(XtX_lambdaI)</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co">#&gt; [1] 200 200</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="fu">qr</span>(XtX_lambdaI)<span class="sc">$</span>rank <span class="sc">==</span> <span class="dv">200</span> <span class="co"># indeed</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
</details>
</div>
<div id="check-that-the-inverse-of-mathbfxtxlambdamathbfi-can-be-computed." class="section level4 unnumbered">
<h4>2. Check that the inverse of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="false" form="prefix" mathvariant="bold">(</mo><msup><mi>ğ—</mi><mi>ğ“</mi></msup><mi>ğ—</mi></mrow><mo>+</mo><mi>Î»</mi><mi>ğˆ</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">{\mathbf{(X^TX}}+\lambda{\mathbf{I}})</annotation></semantics></math> can be computed.</h4>
<details>
<summary>
Solution
</summary>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Yes, it can be computed (no error)</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>XtX_lambdaI_inv <span class="ot">&lt;-</span> <span class="fu">solve</span>(XtX_lambdaI)</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="fu">str</span>(XtX_lambdaI_inv)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co">#&gt;  num [1:200, 1:200] 0.25408 -0.02756 0.00453 -0.02961 0.00722 ...</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="co">#&gt;  - attr(*, &quot;dimnames&quot;)=List of 2</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="co">#&gt;   ..$ : chr [1:200] &quot;1377&quot; &quot;1748&quot; &quot;2487&quot; &quot;2679&quot; ...</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="co">#&gt;   ..$ : chr [1:200] &quot;1377&quot; &quot;1748&quot; &quot;2487&quot; &quot;2679&quot; ...</span></span></code></pre></div>
</details>
</div>
<div id="finally-compute-boldsymbolhatbetamathbfxtxlambdamathbfi-1mathbfxty." class="section level4 unnumbered">
<h4>3. Finally, compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mrow><msup><mi>ğ—</mi><mi>ğ“</mi></msup><mi>ğ—</mi></mrow></mrow><mo>+</mo><mi>Î»</mi><mi>ğˆ</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><mrow><msup><mi>ğ—</mi><mi>ğ“</mi></msup><mi>ğ˜</mi></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\hat{\beta}}=(\mathbf{X^TX}}+\lambda{\mathbf{I}})^{-1}{\mathbf{X^TY}}</annotation></semantics></math>.</h4>
<details>
<summary>
Solution
</summary>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="do">## Calculate ridge beta estimates</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="do">## Use `drop` to drop dimensions and create vector</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>ridge_betas <span class="ot">&lt;-</span> <span class="fu">drop</span>(XtX_lambdaI_inv <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> Y)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="fu">length</span>(ridge_betas) <span class="co"># one for every gene</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="co">#&gt; [1] 200</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a><span class="fu">summary</span>(ridge_betas)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="co">#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. </span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="co">#&gt; -0.235089 -0.045535 -0.008577 -0.000279  0.054195  0.215292</span></span></code></pre></div>
<p>We have now manually calculated the ridge regression estimates.</p>
</details>
</div>
</div>
</div>
<div id="performing-ridge-and-lasso-regression-with-glmnet" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Performing ridge and lasso regression with <code>glmnet</code></h1>
<p>The package <em><a href="https://CRAN.R-project.org/package=glmnet">glmnet</a></em> provides a
function <code>glmnet()</code> that allows you to fit all three types of regressions. Which
type is used, can be determined by specifying the <code>alpha</code> argument. For a
<strong>ridge regression</strong>, you set <code>alpha</code> to 0, and for a <strong>lasso regression</strong> you
set <code>alpha</code> to 1. Other <code>alpha</code> values between 0 and 1 will fit a form of
elastic net. This function has slightly different syntax from the other
model-fitting functions. To be able to use it, you have to pass a <code>x</code> matrix as
well as a <code>y</code> vector, and you donâ€™t use the formula syntax.</p>
<p>The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> parameter, which controls the â€œstrengthâ€ of the penalty, can be
passed by the argument <code>lambda</code>. The function <code>glmnet()</code> can also carry out a
search for finding the best <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> value for a fit. This can be done by
passing multiple values to the argument <code>lambda</code>. If not supplied, <code>glmnet</code> will
generate a range of values itself, based on the data whereby the number of
values can be controlled with the <code>nlambda</code> argument. This is generally the
recommended way to use <code>glmnet</code>, see <code>?glmnet</code> for details.</p>
<p>For a thorough introduction to the <strong>glmnet</strong> package and elastic net models in
general, see the
<a href="https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet.pdf">glmnet introduction vignette</a></p>
<div id="demonstration-ridge-regression" class="section level2 unnumbered">
<h2>Demonstration: Ridge regression</h2>
<p>Letâ€™s perform a ridge regression in order to predict expression levels
of the TRIM32 gene using the 200 gene probes data. We can start by
using a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> value of 2.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>ridge_model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="co"># have a look at the first 10 coefficients</span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="fu">coef</span>(ridge_model)[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co">#&gt;  [1] -1.532831e-14 -5.818717e-03 -9.888023e-03  5.100910e-03 -2.482488e-03</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="co">#&gt;  [6] -8.341285e-03 -4.528922e-03 -7.961890e-03 -5.039029e-03  6.325841e-03</span></span></code></pre></div>
<p>The first coefficient is the intercept, and is again essentially 0. But
a value of 2 for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> might not be the best choice, so letâ€™s see how
the coefficients change with different values for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>.</p>
<p>We will create a <em>grid</em> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> values, i.e.Â a range of values that will be
used as input for the <code>glmnet</code> function. Note that this function can take a
vector of values as input for the <code>lambda</code> argument, allowing to fit multiple
models with the same input data but different hyperparameters. For computational
efficieny, it is recommended to specify the grid as a decreasing sequence.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1000</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="sc">-</span><span class="dv">9</span>) <span class="co"># 1000 to 1 with steps of 9</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>ridge_mod_grid <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> grid)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="co"># Plot the coefficients against the (natural) LOG lambda sequence!</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="co"># see ?plot.glmnet</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a><span class="fu">plot</span>(ridge_mod_grid, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;log(lambda)&quot;</span>)</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="co"># add a vertical line at lambda = 2</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a><span class="fu">text</span>(<span class="fu">log</span>(lambda), <span class="sc">-</span><span class="fl">0.05</span>, <span class="at">labels =</span> <span class="fu">expression</span>(lambda <span class="sc">==</span> <span class="dv">2</span>),</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>     <span class="at">adj =</span> <span class="sc">-</span><span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;firebrick&quot;</span>)</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">log</span>(lambda), <span class="at">col =</span> <span class="st">&quot;firebrick&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="Lab3-Penalized-Regression_files/figure-html/ridge-regression-grid-search-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>This plot is known as a <strong>coefficient profile plot</strong>, each colored line
represents a coefficient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>Î²</mi><mo accent="true">Ì‚</mo></mover><annotation encoding="application/x-tex">\hat{\beta}</annotation></semantics></math> from the regression model and shows how
they change with increased values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> (on the log-scale)
<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>Note that for higher values <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>, the coefficient estimates become closer to 0,
showing the <em>shrinkage</em> effect of the ridge penalty.</p>
<p>Similar to the PC regression example, we chose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\lambda=2</annotation></semantics></math> and the grid rather
arbitrarily. We will see subsequently how to choose the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> that minimizes the
prediction error.</p>
</div>
</div>
<div id="exercise-lasso-regression" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Exercise: Lasso regression</h1>
<p>Lasso regression is also a form of penalized regression, but we do not have an
analytic solution of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><annotation encoding="application/x-tex">\hat{{\boldsymbol{\beta}}}</annotation></semantics></math> as in least squares
and ridge regression. In order to fit a lasso model, we once again use
the <code>glmnet()</code> function. However, this time we use the argument
<code>alpha = 1</code></p>
<div id="tasks-1" class="section level2 unnumbered">
<h2>Tasks</h2>
<div id="perform-a-lasso-regression-with-the-glmnet-function-with-y-the-response-and-x-the-predictors." class="section level4 unnumbered">
<h4>1. Perform a lasso regression with the <code>glmnet</code> function with <code>Y</code> the response and <code>X</code> the predictors.</h4>
<p>You can either provide a custom descending sequence of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> (<code>lambda</code>)
values or instead rely on <code>glmnet</code>â€™s default behaviour of choosing the grid
of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> values based on the data (see <code>?glmnet</code> for more details).</p>
<details>
<summary>
Solution
</summary>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># Note that the glmnet() function can supply lambda automatically</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="co"># By default it uses a sequence of 100 lambda values</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>lasso_model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, Y, <span class="at">alpha =</span> <span class="dv">1</span>)</span></code></pre></div>
</details>
</div>
<div id="make-the-coefficient-profile-plot-and-interpret." class="section level4 unnumbered">
<h4>2. Make the coefficient profile plot and interpret.</h4>
<details>
<summary>
Solution
</summary>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">plot</span>(lasso_model, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;log(lambda)&quot;</span>)</span></code></pre></div>
<p><img src="Lab3-Penalized-Regression_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Note that the number of non-zero coefficients is indicated at the top of the plot.
In the case of lasso-regression the regularization is much less smooth compared
to the ridge regression, with some coefficients increasing for higher <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
before sharply dropping to zero.
In contrast to ridge, lasso eventually shrinks all coefficients to 0.</p>
</details>
</div>
</div>
</div>
<div id="evaluation-of-prediction-models-and-tuning-hyperparameters" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Evaluation of prediction models and tuning hyperparameters</h1>
<p>First we will split our original data in a training and test set to validate our
model. The training set will be used to train the model and tune the
hyperparameters, while the test set will be used to evaluate the
<strong>out-of-sample</strong> performance of our final model. If we would use the same data
to both fit and test the model, we would get biased results.</p>
<p>Before we begin, we use the <code>set.seed()</code> function in order to set a seed
for Râ€™s random number generator, so that we will all obtain precisely
the same results as those shown below. It is generally good practice to
set a random seed when performing an analysis such as cross-validation
that contains an element of randomness, so that the results obtained can
be reproduced at a later time.</p>
<p>We begin by using the <code>sample()</code> function to split the set of samples into two
subsets, by selecting a random subset of 80 observations out of the original 120
observations. We refer to these observations as the <strong>training</strong> set. The rest
of the observations will be used as the <strong>test</strong> set.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="co"># Sample 80 random IDs from the rows of X (120 total)</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>trainID <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(X), <span class="dv">80</span>)</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a><span class="co"># Training data</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>trainX <span class="ot">&lt;-</span> X[trainID, ]</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>trainY <span class="ot">&lt;-</span> Y[trainID]</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a><span class="co"># Test data</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>testX <span class="ot">&lt;-</span> X[<span class="sc">-</span>trainID, ]</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>testY <span class="ot">&lt;-</span> Y[<span class="sc">-</span>trainID]</span></code></pre></div>
<p>To make fitting the models a bit easier later, we will also create 2 data.frames
combining the response and predictors for the training and test data.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;TRIM32&quot;</span> <span class="ot">=</span> trainY, trainX)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>test_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;TRIM32&quot;</span> <span class="ot">=</span> testY, testX)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="do">## Glancing at the data structure: for the first 10 columns only</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a><span class="fu">str</span>(train_data[, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>])</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="co">#&gt; &#39;data.frame&#39;:    80 obs. of  10 variables:</span></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="co">#&gt;  $ TRIM32: num  0.564 0.231 0.215 -0.239 -0.226 ...</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a><span class="co">#&gt;  $ X1377 : num  -0.1498 -0.254 -0.7058 0.1164 -0.0195 ...</span></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="co">#&gt;  $ X1748 : num  -0.3063 -0.4263 -0.4714 -0.5319 -0.0733 ...</span></span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a><span class="co">#&gt;  $ X2487 : num  -0.3588 0.3651 0.0306 0.6907 -1.0774 ...</span></span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a><span class="co">#&gt;  $ X2679 : num  0.2111 -0.0638 -0.0614 -0.129 -0.5161 ...</span></span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a><span class="co">#&gt;  $ X2789 : num  -0.0347 0.8042 -0.0639 0.4437 -0.0162 ...</span></span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a><span class="co">#&gt;  $ X2875 : num  -0.469 -0.535 -0.656 0.134 0.138 ...</span></span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a><span class="co">#&gt;  $ X3244 : num  0.603 -0.639 0.813 -1.101 0.122 ...</span></span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a><span class="co">#&gt;  $ X3375 : num  0.2031 -0.00797 -0.22859 0.45933 -0.11757 ...</span></span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a><span class="co">#&gt;  $ X3732 : num  0.317 -0.517 -0.226 -1.508 0.132 ...</span></span></code></pre></div>
<div id="model-evaluation" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Model evaluation</h2>
<p>We are interested in the <strong>out-of-sample</strong> error of our models,
i.e.Â how good our model does on unseen data.
<strong>This will allow us to compare different <em>classes</em> of models</strong>.
For continuous outcomes we will use the <strong>mean squared error (MSE)</strong>
(or its square-root version, the RMSE).</p>
<p>The evaluation will allow us to compare the performance of different types of
models, e.g.Â PC regression, ridge regression and lasso regression, on our data.
However, we still need to find the optimal model within each of these classes,
by selecting the best hyperparameter (number of PCs for PC regression and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
for lasso and ridge).
For that we will use
<a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)"><em><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold Cross Validation</em></a>
on our training set.</p>
</div>
<div id="tuning-hyperparameters" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Tuning hyperparameters</h2>
<p>The test set is only used to evaluate the <em>final</em> model.
To achieve this final model, we need to find the optimal hyperparameters,
i.e.Â the hyperparameters that best generalize the model to unseen data.
We can estimate this by using <em>k-fold cross validation</em> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><msub><mi>V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">CV_k</annotation></semantics></math>) on
the training data.</p>
<p>The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><msub><mi>V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">CV_k</annotation></semantics></math> estimates can be automatically computed for any
generalized linear model (generated with <code>glm()</code> and by extension <code>glmnet()</code>)
using the <code>cv.glm()</code> function from the
<em><a href="https://CRAN.R-project.org/package=boot">boot</a></em> package.</p>
</div>
</div>
<div id="example-pc-regression-evaluation" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Example: PC regression evaluation</h1>
<p>We start with the PC regression and look for the optimal number of PCs that minimizes
the MSE using <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold Cross validation.
We then use this optimal number of PCs to train the final model and evaluate it
on the test data.</p>
<div id="k-fold-cross-validation-to-tune-number-of-components" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> k-fold Cross Validation to tune number of components</h2>
<p>Conveniently, the <code>pcr</code> function from the <code>pls</code> package has an implementation for
k-fold Cross Validation. We simply need to set <code>validation = CV</code> and <code>segments = 20</code>
to perform 20-fold Cross Validation with PC regression.
If we donâ€™t specify <code>ncomp</code>, <code>pcr</code> will select the maximum number of PCs that can
be used for the CV.</p>
<p>Note that our training data <code>trainX</code> consists of 80 observations (rows).
If we perform 20-fold CV, that means we will split the data in 20 groups, so
each group will consist of 4 observations. At each CV cycle, one group will be left
out and the model will be trained on the remaining groups. This leaves us with
76 training observations for each CV cycle, so the maximal number of components
that can be used in the linear regression is 75.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="do">## Set seed for reproducibility, kCV is a random process!</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a><span class="do">## The &#39;Y ~ .&#39; notation means: fit Y by every other variable in the data</span></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>pcr_cv <span class="ot">&lt;-</span> <span class="fu">pcr</span>(TRIM32 <span class="sc">~</span> ., <span class="at">data =</span> train_data, <span class="at">validation =</span> <span class="st">&quot;CV&quot;</span>, <span class="at">segments =</span> K)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a><span class="fu">summary</span>(pcr_cv)</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a><span class="co">#&gt; Data:    X dimension: 80 200 </span></span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a><span class="co">#&gt;  Y dimension: 80 1</span></span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a><span class="co">#&gt; Fit method: svdpc</span></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a><span class="co">#&gt; Number of components considered: 75</span></span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a><span class="co">#&gt; VALIDATION: RMSEP</span></span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a><span class="co">#&gt; Cross-validated using 20 random segments.</span></span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a><span class="co">#&gt;        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps</span></span>
<span id="cb16-17"><a href="#cb16-17" tabindex="-1"></a><span class="co">#&gt; CV           1.112   0.7013   0.7305   0.7402   0.6939   0.6872   0.6811</span></span>
<span id="cb16-18"><a href="#cb16-18" tabindex="-1"></a><span class="co">#&gt; adjCV        1.112   0.6987   0.7269   0.7375   0.6905   0.6810   0.6759</span></span>
<span id="cb16-19"><a href="#cb16-19" tabindex="-1"></a><span class="co">#&gt;        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps</span></span>
<span id="cb16-20"><a href="#cb16-20" tabindex="-1"></a><span class="co">#&gt; CV      0.6552   0.6812   0.6377    0.6418    0.6153    0.6126    0.6045</span></span>
<span id="cb16-21"><a href="#cb16-21" tabindex="-1"></a><span class="co">#&gt; adjCV   0.6526   0.6772   0.6294    0.6347    0.6080    0.6050    0.5976</span></span>
<span id="cb16-22"><a href="#cb16-22" tabindex="-1"></a><span class="co">#&gt;        14 comps  15 comps  16 comps  17 comps  18 comps  19 comps  20 comps</span></span>
<span id="cb16-23"><a href="#cb16-23" tabindex="-1"></a><span class="co">#&gt; CV       0.6112    0.5882    0.5834    0.5784    0.5744    0.5752    0.5763</span></span>
<span id="cb16-24"><a href="#cb16-24" tabindex="-1"></a><span class="co">#&gt; adjCV    0.6068    0.5788    0.5751    0.5702    0.5669    0.5681    0.5693</span></span>
<span id="cb16-25"><a href="#cb16-25" tabindex="-1"></a><span class="co">#&gt;        21 comps  22 comps  23 comps  24 comps  25 comps  26 comps  27 comps</span></span>
<span id="cb16-26"><a href="#cb16-26" tabindex="-1"></a><span class="co">#&gt; CV       0.5696    0.5654    0.5624    0.5711    0.5676    0.5691    0.5598</span></span>
<span id="cb16-27"><a href="#cb16-27" tabindex="-1"></a><span class="co">#&gt; adjCV    0.5643    0.5608    0.5596    0.5735    0.5586    0.5619    0.5527</span></span>
<span id="cb16-28"><a href="#cb16-28" tabindex="-1"></a><span class="co">#&gt;        28 comps  29 comps  30 comps  31 comps  32 comps  33 comps  34 comps</span></span>
<span id="cb16-29"><a href="#cb16-29" tabindex="-1"></a><span class="co">#&gt; CV       0.5547    0.5441     0.542    0.5444    0.5416    0.5464    0.5497</span></span>
<span id="cb16-30"><a href="#cb16-30" tabindex="-1"></a><span class="co">#&gt; adjCV    0.5491    0.5400     0.539    0.5399    0.5374    0.5426    0.5438</span></span>
<span id="cb16-31"><a href="#cb16-31" tabindex="-1"></a><span class="co">#&gt;        35 comps  36 comps  37 comps  38 comps  39 comps  40 comps  41 comps</span></span>
<span id="cb16-32"><a href="#cb16-32" tabindex="-1"></a><span class="co">#&gt; CV       0.5496    0.5582    0.5618    0.5892    0.5849    0.5929    0.6047</span></span>
<span id="cb16-33"><a href="#cb16-33" tabindex="-1"></a><span class="co">#&gt; adjCV    0.5437    0.5533    0.5561    0.5844    0.5816    0.5882    0.5997</span></span>
<span id="cb16-34"><a href="#cb16-34" tabindex="-1"></a><span class="co">#&gt;        42 comps  43 comps  44 comps  45 comps  46 comps  47 comps  48 comps</span></span>
<span id="cb16-35"><a href="#cb16-35" tabindex="-1"></a><span class="co">#&gt; CV       0.6127    0.6098    0.6230    0.6204    0.6197    0.6299    0.6337</span></span>
<span id="cb16-36"><a href="#cb16-36" tabindex="-1"></a><span class="co">#&gt; adjCV    0.6079    0.6086    0.6214    0.6109    0.6119    0.6211    0.6264</span></span>
<span id="cb16-37"><a href="#cb16-37" tabindex="-1"></a><span class="co">#&gt;        49 comps  50 comps  51 comps  52 comps  53 comps  54 comps  55 comps</span></span>
<span id="cb16-38"><a href="#cb16-38" tabindex="-1"></a><span class="co">#&gt; CV       0.6410    0.6326    0.6375    0.6595    0.6586    0.6599    0.6453</span></span>
<span id="cb16-39"><a href="#cb16-39" tabindex="-1"></a><span class="co">#&gt; adjCV    0.6325    0.6250    0.6322    0.6544    0.6559    0.6548    0.6369</span></span>
<span id="cb16-40"><a href="#cb16-40" tabindex="-1"></a><span class="co">#&gt;        56 comps  57 comps  58 comps  59 comps  60 comps  61 comps  62 comps</span></span>
<span id="cb16-41"><a href="#cb16-41" tabindex="-1"></a><span class="co">#&gt; CV       0.6442    0.6380    0.6456    0.6389    0.6733    0.6544    0.6544</span></span>
<span id="cb16-42"><a href="#cb16-42" tabindex="-1"></a><span class="co">#&gt; adjCV    0.6355    0.6264    0.6361    0.6327    0.6712    0.6555    0.6425</span></span>
<span id="cb16-43"><a href="#cb16-43" tabindex="-1"></a><span class="co">#&gt;        63 comps  64 comps  65 comps  66 comps  67 comps  68 comps  69 comps</span></span>
<span id="cb16-44"><a href="#cb16-44" tabindex="-1"></a><span class="co">#&gt; CV       0.6566    0.6638    0.6770    0.6657    0.6615    0.6697    0.6545</span></span>
<span id="cb16-45"><a href="#cb16-45" tabindex="-1"></a><span class="co">#&gt; adjCV    0.6462    0.6549    0.6685    0.6586    0.6535    0.6634    0.6456</span></span>
<span id="cb16-46"><a href="#cb16-46" tabindex="-1"></a><span class="co">#&gt;        70 comps  71 comps  72 comps  73 comps  74 comps  75 comps</span></span>
<span id="cb16-47"><a href="#cb16-47" tabindex="-1"></a><span class="co">#&gt; CV       0.6435    0.6402    0.6229    0.6313     0.632    0.6192</span></span>
<span id="cb16-48"><a href="#cb16-48" tabindex="-1"></a><span class="co">#&gt; adjCV    0.6361    0.6292    0.6134    0.6241     0.624    0.6113</span></span>
<span id="cb16-49"><a href="#cb16-49" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb16-50"><a href="#cb16-50" tabindex="-1"></a><span class="co">#&gt; TRAINING: % variance explained</span></span>
<span id="cb16-51"><a href="#cb16-51" tabindex="-1"></a><span class="co">#&gt;         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps</span></span>
<span id="cb16-52"><a href="#cb16-52" tabindex="-1"></a><span class="co">#&gt; X         64.80    72.80    75.38    77.61    79.16    80.58    81.86    82.97</span></span>
<span id="cb16-53"><a href="#cb16-53" tabindex="-1"></a><span class="co">#&gt; TRIM32    68.18    68.55    68.58    72.27    76.46    77.04    77.85    79.04</span></span>
<span id="cb16-54"><a href="#cb16-54" tabindex="-1"></a><span class="co">#&gt;         9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps</span></span>
<span id="cb16-55"><a href="#cb16-55" tabindex="-1"></a><span class="co">#&gt; X         83.86     84.66     85.44     86.12     86.77     87.37     87.93</span></span>
<span id="cb16-56"><a href="#cb16-56" tabindex="-1"></a><span class="co">#&gt; TRIM32    83.13     83.27     84.03     84.48     84.78     84.96     86.27</span></span>
<span id="cb16-57"><a href="#cb16-57" tabindex="-1"></a><span class="co">#&gt;         16 comps  17 comps  18 comps  19 comps  20 comps  21 comps  22 comps</span></span>
<span id="cb16-58"><a href="#cb16-58" tabindex="-1"></a><span class="co">#&gt; X          88.45     88.95     89.43     89.90     90.33     90.75     91.14</span></span>
<span id="cb16-59"><a href="#cb16-59" tabindex="-1"></a><span class="co">#&gt; TRIM32     86.27     86.38     86.42     86.43     86.45     86.45     86.56</span></span>
<span id="cb16-60"><a href="#cb16-60" tabindex="-1"></a><span class="co">#&gt;         23 comps  24 comps  25 comps  26 comps  27 comps  28 comps  29 comps</span></span>
<span id="cb16-61"><a href="#cb16-61" tabindex="-1"></a><span class="co">#&gt; X          91.51     91.87     92.22     92.57     92.89     93.20     93.50</span></span>
<span id="cb16-62"><a href="#cb16-62" tabindex="-1"></a><span class="co">#&gt; TRIM32     86.63     86.66     88.26     88.26     88.42     88.45     88.51</span></span>
<span id="cb16-63"><a href="#cb16-63" tabindex="-1"></a><span class="co">#&gt;         30 comps  31 comps  32 comps  33 comps  34 comps  35 comps  36 comps</span></span>
<span id="cb16-64"><a href="#cb16-64" tabindex="-1"></a><span class="co">#&gt; X          93.78     94.05     94.31     94.57     94.82     95.05     95.28</span></span>
<span id="cb16-65"><a href="#cb16-65" tabindex="-1"></a><span class="co">#&gt; TRIM32     88.60     88.89     89.14     89.22     89.49     89.57     89.59</span></span>
<span id="cb16-66"><a href="#cb16-66" tabindex="-1"></a><span class="co">#&gt;         37 comps  38 comps  39 comps  40 comps  41 comps  42 comps  43 comps</span></span>
<span id="cb16-67"><a href="#cb16-67" tabindex="-1"></a><span class="co">#&gt; X          95.50     95.72     95.92     96.13     96.33     96.51     96.69</span></span>
<span id="cb16-68"><a href="#cb16-68" tabindex="-1"></a><span class="co">#&gt; TRIM32     89.83     89.84     89.92     90.20     90.37     90.52     90.60</span></span>
<span id="cb16-69"><a href="#cb16-69" tabindex="-1"></a><span class="co">#&gt;         44 comps  45 comps  46 comps  47 comps  48 comps  49 comps  50 comps</span></span>
<span id="cb16-70"><a href="#cb16-70" tabindex="-1"></a><span class="co">#&gt; X          96.86     97.03     97.19     97.34     97.49     97.63     97.77</span></span>
<span id="cb16-71"><a href="#cb16-71" tabindex="-1"></a><span class="co">#&gt; TRIM32     90.90     91.92     91.93     92.07     92.09     92.29     92.47</span></span>
<span id="cb16-72"><a href="#cb16-72" tabindex="-1"></a><span class="co">#&gt;         51 comps  52 comps  53 comps  54 comps  55 comps  56 comps  57 comps</span></span>
<span id="cb16-73"><a href="#cb16-73" tabindex="-1"></a><span class="co">#&gt; X          97.90     98.03     98.15     98.27     98.38     98.49     98.59</span></span>
<span id="cb16-74"><a href="#cb16-74" tabindex="-1"></a><span class="co">#&gt; TRIM32     92.47     92.53     92.57     93.42     94.03     94.31     94.77</span></span>
<span id="cb16-75"><a href="#cb16-75" tabindex="-1"></a><span class="co">#&gt;         58 comps  59 comps  60 comps  61 comps  62 comps  63 comps  64 comps</span></span>
<span id="cb16-76"><a href="#cb16-76" tabindex="-1"></a><span class="co">#&gt; X          98.70     98.80     98.89     98.98     99.06     99.15     99.23</span></span>
<span id="cb16-77"><a href="#cb16-77" tabindex="-1"></a><span class="co">#&gt; TRIM32     94.82     94.85     94.86     94.89     96.55     96.69     96.72</span></span>
<span id="cb16-78"><a href="#cb16-78" tabindex="-1"></a><span class="co">#&gt;         65 comps  66 comps  67 comps  68 comps  69 comps  70 comps  71 comps</span></span>
<span id="cb16-79"><a href="#cb16-79" tabindex="-1"></a><span class="co">#&gt; X          99.30     99.38     99.45     99.51     99.57     99.63     99.68</span></span>
<span id="cb16-80"><a href="#cb16-80" tabindex="-1"></a><span class="co">#&gt; TRIM32     96.76     96.99     97.40     97.47     97.98     98.00     98.60</span></span>
<span id="cb16-81"><a href="#cb16-81" tabindex="-1"></a><span class="co">#&gt;         72 comps  73 comps  74 comps  75 comps</span></span>
<span id="cb16-82"><a href="#cb16-82" tabindex="-1"></a><span class="co">#&gt; X          99.73     99.78     99.82     99.87</span></span>
<span id="cb16-83"><a href="#cb16-83" tabindex="-1"></a><span class="co">#&gt; TRIM32     98.75     98.77     99.04     99.22</span></span></code></pre></div>
<p>We can plot the <em>root mean squared error of prediction</em> (RMSEP) for each number
of components as follows.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="fu">plot</span>(pcr_cv, <span class="at">plottype =</span> <span class="st">&quot;validation&quot;</span>)</span></code></pre></div>
<p><img src="Lab3-Penalized-Regression_files/figure-html/pcr_cv-plot-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The <code>pls</code> package also has a function <code>selectNcomp</code> to select the optimal number of components.
Here we use the â€œone-sigmaâ€ method, which returns the lowest number of components
for which the RMSE is within one standard error of the absolute minimum.
The function also allows plotting the result by specifying <code>plot = TRUE</code>.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>optimal_ncomp <span class="ot">&lt;-</span> <span class="fu">selectNcomp</span>(pcr_cv, <span class="at">method =</span> <span class="st">&quot;onesigma&quot;</span>, <span class="at">plot =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="Lab3-Penalized-Regression_files/figure-html/pcr-optimal-ncomp-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The optimal number of components for our model is 13.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>optimal_ncomp</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a><span class="co">#&gt; [1] 13</span></span></code></pre></div>
</div>
<div id="validation-on-test-data" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Validation on test data</h2>
<p>We now use our optimal number of components to train the final PCR model.
This model is then validated on by generating predictions for the test data and
calculating the MSE.</p>
<p>We define a custom function to calculate the MSE.
Note that there is also an <code>MSEP</code> function in the <code>pls</code> package which does the
prediction and MSE calculation in one go.
But our own function will come in handy later for lasso and ridge regression.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># Mean Squared Error</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="do">## obs: observations; pred: predictions</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>MSE <span class="ot">&lt;-</span> <span class="cf">function</span>(obs, pred){</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>  <span class="fu">mean</span>((<span class="fu">drop</span>(obs) <span class="sc">-</span> <span class="fu">drop</span>(pred))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>final_pcr_model <span class="ot">&lt;-</span> <span class="fu">pcr</span>(TRIM32 <span class="sc">~</span> ., <span class="at">data =</span> train_data, <span class="at">ncomp =</span> optimal_ncomp)</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>pcr_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(final_pcr_model, <span class="at">newdata =</span> test_data, <span class="at">ncomp =</span> optimal_ncomp)</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>(pcr_mse <span class="ot">&lt;-</span> <span class="fu">MSE</span>(testY, pcr_preds))</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a><span class="co">#&gt; [1] 0.3655052</span></span></code></pre></div>
<p>This value on its own does not tell us very much, but we can use it to compare our
PCR model with other types of models later.</p>
<p>Finally, we plot the predicted values for our response variable (the TRIM32 gene expression)
against the actual observed values from our test set.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="fu">predplot</span>(final_pcr_model, <span class="at">newdata =</span> test_data, <span class="at">line =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="Lab3-Penalized-Regression_files/figure-html/pcr-predplot-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="exercise-evaluate-and-compare-prediction-models" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Exercise: evaluate and compare prediction models</h1>
<div id="perform-a-lasso-regression-with-20-fold-cross-validation-on-the-training-data-trainx-trainy.-plot-the-results-and-select-the-optimal-lambda-parameter.-fit-a-final-model-with-the-selected-lambda-and-validate-it-on-the-test-data." class="section level4 unnumbered">
<h4>1. Perform a lasso regression with 20-fold Cross Validation on the training data (<code>trainX</code>, <code>trainY</code>). Plot the results and select the optimal <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> parameter. Fit a final model with the selected <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> and validate it on the test data.</h4>
<p><em>Hint</em>: use the <code>cv.glmnet()</code> function, for 20 folds CV, set <code>nfolds = 20</code> and
to use the MSE metric set <code>type.measure = "mse"</code>.
Go to <code>?cv.glmnet</code> for details.</p>
<details>
<summary>
Solution
</summary>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>lasso_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(trainX, trainY, <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>                      <span class="at">nfolds =</span> K, <span class="at">type.measure =</span> <span class="st">&quot;mse&quot;</span>)</span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>lasso_cv</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a><span class="co">#&gt; Call:  cv.glmnet(x = trainX, y = trainY, type.measure = &quot;mse&quot;, nfolds = K,      alpha = 1) </span></span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a><span class="co">#&gt; Measure: Mean-Squared Error </span></span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb23-10"><a href="#cb23-10" tabindex="-1"></a><span class="co">#&gt;      Lambda Index Measure     SE Nonzero</span></span>
<span id="cb23-11"><a href="#cb23-11" tabindex="-1"></a><span class="co">#&gt; min 0.07559    55  0.3639 0.0750      16</span></span>
<span id="cb23-12"><a href="#cb23-12" tabindex="-1"></a><span class="co">#&gt; 1se 0.16668    38  0.4353 0.1646       9</span></span>
<span id="cb23-13"><a href="#cb23-13" tabindex="-1"></a><span class="fu">plot</span>(lasso_cv)</span></code></pre></div>
<p><img src="Lab3-Penalized-Regression_files/figure-html/lasso-cv-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Note that we can extract the fitted lasso regression object from the CV result
and make the coefficient profile plot as before.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="fu">plot</span>(lasso_cv<span class="sc">$</span>glmnet.fit, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="Lab3-Penalized-Regression_files/figure-html/lasso-cv-coefficient-profile-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We can look for the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> values that give the best result.
Here you have two possibilities :</p>
<ol style="list-style-type: decimal">
<li><code>lambda.min</code>: the value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> that gives the best result for the crossvalidation.</li>
<li><code>lambda.1se</code>: the largest value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> such that the MSE is within 1 standard error
of the best result from the cross validation.</li>
</ol>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>lasso_cv<span class="sc">$</span>lambda.min</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a><span class="co">#&gt; [1] 0.07558811</span></span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>lasso_cv<span class="sc">$</span>lambda<span class="fl">.1</span>se</span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a><span class="co">#&gt; [1] 0.1666817</span></span></code></pre></div>
<p>We will use <code>lambda.min</code> here to fit the final model and generate predictions on the test data.
Note that we donâ€™t actually have to redo the fitting, we can just use our existing
<code>lasso_cv</code> object, which already contains the fitted models for a range of <code>lambda</code> values.
We can use the <code>predict</code> function and specify the <code>s</code> argument (which confusingly sets <code>lambda</code> in this case) to make predictions on the test data.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>lasso_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso_cv, <span class="at">s =</span> lasso_cv<span class="sc">$</span>lambda.min, <span class="at">newx =</span> testX)</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a><span class="do">## Calculate MSE</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>(lasso_mse <span class="ot">&lt;-</span> <span class="fu">MSE</span>(testY, lasso_preds))</span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a><span class="co">#&gt; [1] 0.3754368</span></span></code></pre></div>
</details>
</div>
<div id="do-the-same-for-ridge-regression." class="section level4 unnumbered">
<h4>2. Do the same for ridge regression.</h4>
<details>
<summary>
Solution
</summary>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a>ridge_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(trainX, trainY, <span class="at">alpha =</span> <span class="dv">0</span>,</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>                      <span class="at">nfolds =</span> K, <span class="at">type.measure =</span> <span class="st">&quot;mse&quot;</span>)</span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a>ridge_cv</span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a><span class="co">#&gt; Call:  cv.glmnet(x = trainX, y = trainY, type.measure = &quot;mse&quot;, nfolds = K,      alpha = 0) </span></span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a><span class="co">#&gt; Measure: Mean-Squared Error </span></span>
<span id="cb27-9"><a href="#cb27-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb27-10"><a href="#cb27-10" tabindex="-1"></a><span class="co">#&gt;     Lambda Index Measure     SE Nonzero</span></span>
<span id="cb27-11"><a href="#cb27-11" tabindex="-1"></a><span class="co">#&gt; min   9.32   100  0.4648 0.1182     200</span></span>
<span id="cb27-12"><a href="#cb27-12" tabindex="-1"></a><span class="co">#&gt; 1se  43.25    67  0.5820 0.2131     200</span></span>
<span id="cb27-13"><a href="#cb27-13" tabindex="-1"></a><span class="fu">plot</span>(ridge_cv)</span></code></pre></div>
<p><img src="Lab3-Penalized-Regression_files/figure-html/ridge-cv-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Since the MSE is minimized at the smallest considered <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mo>=</mo></mrow><annotation encoding="application/x-tex">\lambda =</annotation></semantics></math> 9.3188551,
we should extend the grid to include smaller values than those that were chosen by the
default setting of <code>cv.glmnet()</code>. Intuitively, this is because the MSE might continue to
decrease beyond the left boundary of the plot.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>ridge_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(trainX, trainY, <span class="at">alpha =</span> <span class="dv">0</span>,</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>                      <span class="at">nfolds =</span> K, <span class="at">type.measure =</span> <span class="st">&quot;mse&quot;</span>,</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>                      <span class="at">lambda =</span> <span class="fu">exp</span>(<span class="fu">seq</span>(<span class="dv">7</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">by =</span> <span class="sc">-</span><span class="fl">0.1</span>)))</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>ridge_cv</span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a><span class="co">#&gt; Call:  cv.glmnet(x = trainX, y = trainY, lambda = exp(seq(7, -2, by = -0.1)),      type.measure = &quot;mse&quot;, nfolds = K, alpha = 0) </span></span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a><span class="co">#&gt; Measure: Mean-Squared Error </span></span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb28-11"><a href="#cb28-11" tabindex="-1"></a><span class="co">#&gt;     Lambda Index Measure      SE Nonzero</span></span>
<span id="cb28-12"><a href="#cb28-12" tabindex="-1"></a><span class="co">#&gt; min  0.549    77  0.3475 0.06304     200</span></span>
<span id="cb28-13"><a href="#cb28-13" tabindex="-1"></a><span class="co">#&gt; 1se  3.320    59  0.4057 0.09027     200</span></span>
<span id="cb28-14"><a href="#cb28-14" tabindex="-1"></a><span class="fu">plot</span>(ridge_cv)</span></code></pre></div>
<p><img src="Lab3-Penalized-Regression_files/figure-html/ridge-cv-alt-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Note that we can extract the fitted ridge regression object from the CV result
and make the coefficient profile plot as before.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="fu">plot</span>(ridge_cv<span class="sc">$</span>glmnet.fit, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="Lab3-Penalized-Regression_files/figure-html/ridge-cv-coefficient-profile-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We can look for the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> values that give the best result.
Here you have two possibilities :</p>
<ol style="list-style-type: decimal">
<li><code>lambda.min</code>: the value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> that gives the best result for the crossvalidation.</li>
<li><code>lambda.1se</code>: the largest value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î»</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> such that the MSE is within 1 standard error
of the best result from the cross validation.</li>
</ol>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>ridge_cv<span class="sc">$</span>lambda.min</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a><span class="co">#&gt; [1] 0.5488116</span></span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a>ridge_cv<span class="sc">$</span>lambda<span class="fl">.1</span>se</span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a><span class="co">#&gt; [1] 3.320117</span></span></code></pre></div>
<p>We will use <code>lambda.min</code> here to fit the final model and generate predictions on the test data.
Note that we donâ€™t actually have to redo the fitting, we can just use our existing
<code>ridge_cv</code> object, which already contains the fitted models for a range of <code>lambda</code> values.
We can use the <code>predict</code> function and specify the <code>s</code> argument (which confusingly sets <code>lambda</code> in this case) to make predictions on the test data.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>ridge_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge_cv, <span class="at">s =</span> ridge_cv<span class="sc">$</span>lambda.min, <span class="at">newx =</span> testX)</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a><span class="do">## Calculate MSE</span></span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>(ridge_mse <span class="ot">&lt;-</span> <span class="fu">MSE</span>(testY, ridge_preds))</span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a><span class="co">#&gt; [1] 0.3170542</span></span></code></pre></div>
</details>
</div>
<div id="which-of-the-models-considered-pcr-lasso-ridge-performs-best." class="section level4 unnumbered">
<h4>3. Which of the models considered (PCR, lasso, ridge) performs best?.</h4>
<details>
<summary>
Solution
</summary>
<p>Based on the MSE, the ridge model performs best on the test data.</p>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="right">MSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">PCR</td>
<td align="right">0.3655052</td>
</tr>
<tr class="even">
<td align="left">Lasso</td>
<td align="right">0.3754368</td>
</tr>
<tr class="odd">
<td align="left">Ridge</td>
<td align="right">0.3170542</td>
</tr>
</tbody>
</table>
</details>
</div>
</div>
<div id="session-info" class="section level1 unnumbered">
<h1>Session info</h1>
<details>
<summary>
Session info
</summary>
<pre><code>#&gt; [1] &quot;2025-11-06 09:10:15 CET&quot;
#&gt; â”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#&gt;  setting  value
#&gt;  version  R version 4.5.2 (2025-10-31)
#&gt;  os       macOS Sequoia 15.6
#&gt;  system   aarch64, darwin20
#&gt;  ui       X11
#&gt;  language (EN)
#&gt;  collate  en_US.UTF-8
#&gt;  ctype    en_US.UTF-8
#&gt;  tz       Europe/Brussels
#&gt;  date     2025-11-06
#&gt;  pandoc   3.6.3 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/aarch64/ (via rmarkdown)
#&gt;  quarto   1.7.32 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/quarto
#&gt; 
#&gt; â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#&gt;  package     * version date (UTC) lib source
#&gt;  bookdown      0.45    2025-10-03 [1] CRAN (R 4.5.0)
#&gt;  boot        * 1.3-32  2025-08-29 [1] CRAN (R 4.5.2)
#&gt;  bslib         0.9.0   2025-01-30 [1] CRAN (R 4.5.0)
#&gt;  cachem        1.1.0   2024-05-16 [1] CRAN (R 4.5.0)
#&gt;  cli           3.6.5   2025-04-23 [1] CRAN (R 4.5.0)
#&gt;  codetools     0.2-20  2024-03-31 [1] CRAN (R 4.5.2)
#&gt;  digest        0.6.37  2024-08-19 [1] CRAN (R 4.5.0)
#&gt;  evaluate      1.0.5   2025-08-27 [1] CRAN (R 4.5.0)
#&gt;  fastmap       1.2.0   2024-05-15 [1] CRAN (R 4.5.0)
#&gt;  foreach       1.5.2   2022-02-02 [1] CRAN (R 4.5.0)
#&gt;  glmnet      * 4.1-10  2025-07-17 [1] CRAN (R 4.5.0)
#&gt;  HDDAData    * 1.0.1   2025-11-06 [1] Github (statOmics/HDDAData@b832c71)
#&gt;  htmltools     0.5.8.1 2024-04-04 [1] CRAN (R 4.5.0)
#&gt;  iterators     1.0.14  2022-02-05 [1] CRAN (R 4.5.0)
#&gt;  jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.5.0)
#&gt;  jsonlite      2.0.0   2025-03-27 [1] CRAN (R 4.5.0)
#&gt;  knitr         1.50    2025-03-16 [1] CRAN (R 4.5.0)
#&gt;  lattice       0.22-7  2025-04-02 [1] CRAN (R 4.5.2)
#&gt;  lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.5.0)
#&gt;  Matrix      * 1.7-4   2025-08-28 [1] CRAN (R 4.5.2)
#&gt;  pls         * 2.8-5   2024-09-15 [1] CRAN (R 4.5.0)
#&gt;  R6            2.6.1   2025-02-15 [1] CRAN (R 4.5.0)
#&gt;  Rcpp          1.1.0   2025-07-02 [1] CRAN (R 4.5.0)
#&gt;  rlang         1.1.6   2025-04-11 [1] CRAN (R 4.5.0)
#&gt;  rmarkdown     2.30    2025-09-28 [1] CRAN (R 4.5.0)
#&gt;  rstudioapi    0.17.1  2024-10-22 [1] CRAN (R 4.5.0)
#&gt;  sass          0.4.10  2025-04-11 [1] CRAN (R 4.5.0)
#&gt;  sessioninfo   1.2.3   2025-02-05 [1] CRAN (R 4.5.0)
#&gt;  shape         1.4.6.1 2024-02-23 [1] CRAN (R 4.5.0)
#&gt;  survival      3.8-3   2024-12-17 [1] CRAN (R 4.5.2)
#&gt;  xfun          0.54    2025-10-30 [1] CRAN (R 4.5.0)
#&gt;  yaml          2.3.10  2024-07-26 [1] CRAN (R 4.5.0)
#&gt; 
#&gt;  [1] /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library
#&gt;  * â”€â”€ Packages attached to the search path.
#&gt; 
#&gt; â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</code></pre>
</details>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Note: <code>log()</code> in R is the <strong>natural logarithm</strong> by default (base <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>) and we
will also use this notation in the text (like the x-axis title on the plot above).
This might be different from the notation that youâ€™re used to (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ln()</annotation></semantics></math>).
To take logarithms with a different base in R you can specify the <code>base =</code>
argument of <code>log</code> or use the shorthand functions <code>log10(x)</code> and <code>log2(x)</code> for
base 10 and 2, respectively<a href="#fnref1" class="footnote-back">â†©ï¸</a></p></li>
<li id="fn2"><p>Note: The solid black line indicates the cross-validated RMSEP,
while the dashed red line adjusts the RMSEP estimate downwards to account for the fact
that the PCR is trained on only <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>k</mi><mo>âˆ’</mo><mn>1</mn></mrow><mi>k</mi></mfrac><msub><mi>n</mi><mtext mathvariant="normal">train</mtext></msub><mo>=</mo><mn>76</mn></mrow><annotation encoding="application/x-tex">\frac{k-1}{k} n_{\text{train}} = 76</annotation></semantics></math> instead of the full
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mtext mathvariant="normal">train</mtext></msub><mo>=</mo><mn>80</mn></mrow><annotation encoding="application/x-tex">n_{\text{train}} = 80</annotation></semantics></math> observations. See Equation (5) of
<a href="https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.887"><em>Mevik and Cederkvist, 2005</em></a>
for the definition of the adjustment. If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> is relatively large, such as our <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">k = 20</annotation></semantics></math>,
the adjustment does not have a large impact and the solid and dashed lines are quite
similar.<a href="#fnref2" class="footnote-back">â†©ï¸</a></p></li>
</ol>
</div>

<div id="rmd-source-code">---
title: "Lab 3: Penalized regression techniques for high-dimensional data"
subtitle: "High Dimensional Data Analysis practicals"
author: "Adapted by Milan Malfait and Leo Fuhrhop"
date: "04 Nov 2021 <br/> (Last updated: 2025-11-06)"
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  out.width = "100%"
)
options(
  warnPartialMatchDollar = FALSE,
  warnPartialMatchAttr = FALSE,
  warnPartialMatchArgs = FALSE
)
```

### [Change log](https://github.com/statOmics/HDDA/commits/master/Lab3-Penalized-Regression.Rmd) {-}

***

```{r libraries, warning=FALSE, message=FALSE}
## install packages with:
# install.packages(c("glmnet", "pls", "boot"))
# remotes::install_github("statOmics/HDDAData")
library(HDDAData)
library(glmnet)
library(pls)
library(boot)
```


# Introduction

**In this lab session we will look at the following topics**

  - Demonstrate why low-dimensional prediction modeling fails in high-dimensional settings.
  - Carry out Principal Component Regression (PCR).
  - Use `glmnet()` to carry out ridge regression, lasso and elastic net.
  - Evaluate these trained prediction models.


## The dataset

In this practical, we will use the dataset `eyedata` provided by
the [__NormalBetaPrime__ package](https://cran.r-project.org/web/packages/NormalBetaPrime/index.html).
This dataset contains gene expression data of 200
genes for 120 samples. The data originates from microarray experiments
of mammalian eye tissue samples.

The dataset consists of two objects:

  - `genes`: a $120 \times 200$ matrix with the expression levels of 200 genes
  (columns) for 120 samples (rows)
  - `trim32`: a vector with 120 expression levels of the TRIM32 gene.


```{r load-data}
data(eyedata)
genes <- eyedata$genes
trim32 <- eyedata$trim32

## Look at objects that were just loaded
str(genes)
str(trim32)
```

The goal of this exercise is to predict the expression levels of
TRIM32 from the expression levels of the 200 genes measured in the
microarray experiment. For this, it makes sense to start by constructing
centered (and possibly scaled) data. We store this in two matrices
`X` and `Y`:

```{r prepare-data}
X <- scale(genes, center = TRUE, scale = TRUE)
Y <- scale(trim32, center = TRUE)
```

Remember that scaling avoids that differences in levels of magnitude
will give one variable (gene) more influence in the result. This has
been illustrated in the [second practical session](./Lab2-PCA.html) as well.
For the `Y` vector, this is less of an issue as we're talking about a single variable.
Not scaling will make the predictions interpretable as "deviations from the
mean".

## The curse of singularity

We begin by assuming that the predictors and the outcome have been
centered so that the intercept is 0.
We are presented with the usual regression model:

\begin{align*}
&Y_i=\beta_i X_{i1}+\dots+\beta_pX_{ip}+\epsilon_i \\
&\text{ Or } \mathbf{Y}={\mathbf{X}}{\boldsymbol{\beta}} +{\boldsymbol{\epsilon}}
\end{align*}

Our goal is to get the least squares estimator of
${\boldsymbol{\beta}}$, given by

$$
\hat{{\boldsymbol{\beta}}}= (\mathbf{X}^T{\mathbf{X}})^{-1}{\mathbf{X}}^T{\mathbf{Y}}
$$

in which the $p \times p$ matrix
$({\mathbf{X}}^T{\mathbf{X}})^{-1}$ is crucial!
To be able to calculate the inverse of ${\mathbf{X}}^T \mathbf{X}$,
it has to be of full rank $p$, which would be 200 in this case.
Let's check this:

```{r singularity-problem, error=TRUE}
dim(X) # 120 x 200, so p > n!
qr(X)$rank

XtX <- crossprod(X) # calculates t(X) %*% X more efficiently
qr(XtX)$rank

# Try to invert using solve:
solve(XtX)
```

We realize we cannot compute
$({\mathbf{X}}^T{\mathbf{X}})^{-1}$ because the rank of
$({\mathbf{X}}^T{\mathbf{X}})$ is less than $p$ hence we can’t
get $\hat{{\boldsymbol{\beta}}}$ by means of least squares!
This is generally referred to as the __[singularity](https://www.statistics.com/glossary/singularity/) problem__.


# Principal component regression

A first way to deal with this singularity, is to bypass it using principal components.
Since $\min(n,p) = n = 120$,
PCA will give `r min(dim(X))` components, each being a linear combination of the
$p$ = `r ncol(X)` variables.
These `r min(dim(X))` PCs contain all information present in the original data.
We could as well use an approximation of ${\mathbf{X}}$, i.e using just a few ($k<120$) PCs.
So we use PCA as a method for reducing the dimensions while retaining
as much variation between the observations as possible.
Once we have these PCs, we can use them as variables in a linear regression model.

## Classic linear regression on PCs

We first compute the PCA on the data with `prcomp`.
We will use an arbitrary cutoff of $k = 4$ PCs to illustrate the process of performing regression on the PCs.

```{r PC-regression}
k <- 4 # Arbitrarily chosen k=4
pca <- prcomp(X)
Vk <- pca$rotation[, 1:k] # the loadings matrix
Zk <- pca$x[, 1:k] # the scores matrix

# Use the scores in classic linear regression
pcr_model1 <- lm(Y ~ Zk)
summary(pcr_model1)
```

As $\mathbf{X}$ and $\mathbf{Y}$ are centered, the intercept is
approximately 0.

The output shows that PC1 and PC4 have a $\beta$ estimate that
differs significantly from 0 (at $p < 0.05$), but the results can't be readily
interpreted, since we have no immediate interpretation of the PCs.


## Using the package `pls`

PCR can also be performed using the `pcr()` function from the
package *[pls](https://CRAN.R-project.org/package=pls)*
__directly on the data__ (so without having to first perform the PCA manually).
When using this function, you have to keep a few things in mind:

  1. the number of components (PCs) to use is passed with the argument `ncomp`
  2. the function allows you to scale (set `scale = TRUE`) and
  center (set `center = TRUE`) the predictors first (in the example here, $\mathbf{X}$ has already been centered and scaled).

You can use the function `pcr()` in much the same way as you would
use `lm()`. The resulting fit can easily be examined using the
function `summary()`, but the output looks quite different from
what you would get from `lm`.

```{r PC-regression-pls-package}
# X is already scaled and centered, so that's not needed.
pcr_model2 <- pcr(Y ~ X, ncomp = 4)
summary(pcr_model2)
```

First of all the output shows you the data dimensions and the fitting
method used. In this case, that is PC calculation based on SVD. The
`summary()` function also provides the percentage of variance
explained in the predictors and in the response using different numbers
of components. For example, the first PC only captures 61.22% of all
the variance, or information in the predictors and it explains 62.9%
of the variance in the outcome. Note that for both methods the choice of
the number of principal components was arbitrary chosen to be 4.

At a later stage, we will look at how to choose the number of components
that has the __smallest prediction error__.


# Ridges, Lassos and Elastic Nets {#elnet-theory}

Ridge regression, lasso regression and elastic nets are all closely
related techniques, based on the same idea: add a penalty term to
the estimating function so $({\mathbf{X}}^T{\mathbf{X}})$
becomes full rank again and is invertible. Two different penalty
terms or regularization methods can be used:

1. L1 regularization: this regularization adds a term ${\lambda\|\boldsymbol{\beta}\|_{1}}$ to the least squares criterion.
The term will add a penalty based on the *absolute value* of the
magnitude of the coefficients. This is used by __lasso regression__.

$$
 \hat{\boldsymbol{\beta}}^{\text{lasso}} = \text{argmin}_{\boldsymbol{\beta}}\displaystyle({(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^T(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+{\lambda\|\boldsymbol{\beta}\|_{1}}}\displaystyle)
$$

2. L2 regularization: this regularization adds a term ${\lambda\|\boldsymbol{\beta}\|_{2}^{2}}$ to the least squares criterion.
The penalty term is based on the square of the magnitude of the
coefficients. This is used by __ridge regression__.

$$
 \hat{\boldsymbol{\beta}}^{\text{ridge}} = \text{argmin}_{\boldsymbol{\beta}}\displaystyle({(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^T(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+{\lambda\|\boldsymbol{\beta}\|_{2}^{2}}}\displaystyle)
$$

Elastic net regression combines both types of regularization. It does so by
introducing a mixing parameter $\alpha \in [0, 1]$ that essentially combines
the L1 and L2 norms in a weighted average.

$$
 \hat{\boldsymbol{\beta}}^{\text{el.net}} = \text{argmin}_{\boldsymbol{\beta}}\displaystyle({(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^{T}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+{\alpha \lambda\|\boldsymbol{\beta}\|_{1}}+ {(1 - \alpha)\lambda\|\boldsymbol{\beta}\|_{2}^{2}}}\displaystyle)
$$



# Exercise: Verification of ridge regression

In least square regression the minimization of the estimation function
$|{\mathbf{Y} - \mathbf{X} \boldsymbol{\beta}}\|^{2}_{2}$ leads to the solution ${\boldsymbol{\hat{\beta}}=(\mathbf{X^TX})^{-1}\mathbf{X^TY}}$.

For the penalized least squares criterion used by ridge regression, you minimize
$\|{\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}\|^{2}_{2}}+\lambda{\boldsymbol{\|\beta\|^{2}_{2}}}$
which leads to following solution:

$$
{\boldsymbol{\hat{\beta}}=(\mathbf{X^TX}}+\lambda{\mathbf{I}})^{-1}{\mathbf{X^TY}}
$$

where $\mathbf{I}$ is the $p \times p$ identity matrix.

The ridge parameter $\lambda$ *shrinks* the coefficients towards 0, with $\lambda = 0$ being equivalent to OLS (no shrinkage) and $\lambda = +\infty$ being equivalent to setting all $\hat{\beta}$'s to 0.
The optimal parameter lies somewhere in between and needs to be tuned by the user.


## Tasks {-}

Solve the following exercises using R.

#### 1. Verify that ${\mathbf{(X^TX}}+\lambda{\mathbf{I}})$ has rank $200$, for any $\lambda>0$ of your choice. {-}

<details><summary>Solution</summary>
```{r}
XtX <- crossprod(X)
p <- ncol(X)
lambda <- 2 # My choice

# Compute penalized matrix
XtX_lambdaI <- XtX + (lambda * diag(p))
dim(XtX_lambdaI)
qr(XtX_lambdaI)$rank == 200 # indeed
```
</details>


#### 2. Check that the inverse of ${\mathbf{(X^TX}}+\lambda{\mathbf{I}})$ can be computed. {-}

<details><summary>Solution</summary>
```{r}
# Yes, it can be computed (no error)
XtX_lambdaI_inv <- solve(XtX_lambdaI)
str(XtX_lambdaI_inv)
```
</details>


#### 3. Finally, compute ${\boldsymbol{\hat{\beta}}=(\mathbf{X^TX}}+\lambda{\mathbf{I}})^{-1}{\mathbf{X^TY}}$. {-}

<details><summary>Solution</summary>
```{r ridge-beta-estimates}
## Calculate ridge beta estimates
## Use `drop` to drop dimensions and create vector
ridge_betas <- drop(XtX_lambdaI_inv %*% t(X) %*% Y)
length(ridge_betas) # one for every gene
summary(ridge_betas)
```

We have now manually calculated the ridge regression estimates.

</details>



# Performing ridge and lasso regression with `glmnet`

The package *[glmnet](https://CRAN.R-project.org/package=glmnet)* provides a
function `glmnet()` that allows you to fit all three types of regressions. Which
type is used, can be determined by specifying the `alpha` argument. For a
__ridge regression__, you set `alpha` to 0, and for a __lasso regression__ you
set `alpha` to 1. Other `alpha` values between 0 and 1 will fit a form of
elastic net. This function has slightly different syntax from the other
model-fitting functions. To be able to use it, you have to pass a `x` matrix as
well as a `y` vector, and you don't use the formula syntax.

The $\lambda$ parameter, which controls the "strength" of the penalty, can be
passed by the argument `lambda`. The function `glmnet()` can also carry out a
search for finding the best $\lambda$ value for a fit. This can be done by
passing multiple values to the argument `lambda`. If not supplied, `glmnet` will
generate a range of values itself, based on the data whereby the number of
values can be controlled with the `nlambda` argument. This is generally the
recommended way to use `glmnet`, see `?glmnet` for details.

For a thorough introduction to the __glmnet__ package and elastic net models in
general, see the
[glmnet introduction vignette](https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet.pdf)


## Demonstration: Ridge regression {-}

Let's perform a ridge regression in order to predict expression levels
of the TRIM32 gene using the 200 gene probes data. We can start by
using a $\lambda$ value of 2.

```{r glmnet-ridge-regression}
lambda <- 2
ridge_model <- glmnet(X, Y, alpha = 0, lambda = lambda)

# have a look at the first 10 coefficients
coef(ridge_model)[1:10]
```

The first coefficient is the intercept, and is again essentially 0. But
a value of 2 for $\lambda$ might not be the best choice, so let's see how
the coefficients change with different values for $\lambda$.

We will create a *grid* of $\lambda$ values, i.e. a range of values that will be
used as input for the `glmnet` function. Note that this function can take a
vector of values as input for the `lambda` argument, allowing to fit multiple
models with the same input data but different hyperparameters. For computational
efficieny, it is recommended to specify the grid as a decreasing sequence.

```{r ridge-regression-grid-search}
grid <- seq(1000, 1, by = -9) # 1000 to 1 with steps of 9
ridge_mod_grid <- glmnet(X, Y, alpha = 0, lambda = grid)

# Plot the coefficients against the (natural) LOG lambda sequence!
# see ?plot.glmnet
plot(ridge_mod_grid, xvar = "lambda", xlab = "log(lambda)")
# add a vertical line at lambda = 2
text(log(lambda), -0.05, labels = expression(lambda == 2),
     adj = -0.5, col = "firebrick")
abline(v = log(lambda), col = "firebrick", lwd = 2)
```

This plot is known as a __coefficient profile plot__, each colored line
represents a coefficient $\hat{\beta}$ from the regression model and shows how
they change with increased values of $\lambda$ (on the log-scale)
^[Note: `log()` in R is the __natural logarithm__ by default (base $e$) and we
will also use this notation in the text (like the x-axis title on the plot above).
This might be different from the notation that you're used to ($\ln()$).
To take logarithms with a different base in R you can specify the `base = `
argument of `log` or use the shorthand functions `log10(x)` and `log2(x)` for
base 10 and 2, respectively].

Note that for higher values $\lambda$, the coefficient estimates become closer to 0,
showing the *shrinkage* effect of the ridge penalty.

Similar to the PC regression example, we chose $\lambda=2$ and the grid rather
arbitrarily. We will see subsequently how to choose the $\lambda$ that minimizes the
prediction error.


# Exercise: Lasso regression

Lasso regression is also a form of penalized regression, but we do not have an
analytic solution of $\hat{{\boldsymbol{\beta}}}$ as in least squares
and ridge regression. In order to fit a lasso model, we once again use
the `glmnet()` function. However, this time we use the argument
`alpha = 1`


## Tasks {-}

#### 1. Perform a lasso regression with the `glmnet` function with `Y` the response and `X` the predictors. {-}

You can either provide a custom descending sequence of $\lambda$ (`lambda`)
values or instead rely on `glmnet`'s default behaviour of choosing the grid 
of $\lambda$ values based on the data (see `?glmnet` for more details).

<details><summary>Solution</summary>
```{r glmnet-lasso-regression}
# Note that the glmnet() function can supply lambda automatically
# By default it uses a sequence of 100 lambda values
lasso_model <- glmnet(X, Y, alpha = 1)
```
</details>


#### 2. Make the coefficient profile plot and interpret. {-}

<details><summary>Solution</summary>

```{r}
plot(lasso_model, xvar = "lambda", xlab = "log(lambda)")
```

Note that the number of non-zero coefficients is indicated at the top of the plot.
In the case of lasso-regression the regularization is much less smooth compared
to the ridge regression, with some coefficients increasing for higher $\lambda$
before sharply dropping to zero.
In contrast to ridge, lasso eventually shrinks all coefficients to 0.

</details>


# Evaluation of prediction models and tuning hyperparameters

First we will split our original data in a training and test set to validate our
model. The training set will be used to train the model and tune the
hyperparameters, while the test set will be used to evaluate the
__out-of-sample__ performance of our final model. If we would use the same data
to both fit and test the model, we would get biased results.

Before we begin, we use the `set.seed()` function in order to set a seed
for R’s random number generator, so that we will all obtain precisely
the same results as those shown below. It is generally good practice to
set a random seed when performing an analysis such as cross-validation
that contains an element of randomness, so that the results obtained can
be reproduced at a later time.

We begin by using the `sample()` function to split the set of samples into two
subsets, by selecting a random subset of 80 observations out of the original 120
observations. We refer to these observations as the __training__ set. The rest
of the observations will be used as the __test__ set.

```{r create-training-set}
set.seed(1)
# Sample 80 random IDs from the rows of X (120 total)
trainID <- sample(nrow(X), 80)

# Training data
trainX <- X[trainID, ]
trainY <- Y[trainID]

# Test data
testX <- X[-trainID, ]
testY <- Y[-trainID]
```

To make fitting the models a bit easier later, we will also create 2 data.frames
combining the response and predictors for the training and test data.

```{r}
train_data <- data.frame("TRIM32" = trainY, trainX)
test_data <- data.frame("TRIM32" = testY, testX)

## Glancing at the data structure: for the first 10 columns only
str(train_data[, 1:10])
```


## Model evaluation

We are interested in the __out-of-sample__ error of our models,
i.e. how good our model does on unseen data.
__This will allow us to compare different *classes* of models__.
For continuous outcomes we will use the __mean squared error (MSE)__
(or its square-root version, the RMSE).

The evaluation will allow us to compare the performance of different types of
models, e.g. PC regression, ridge regression and lasso regression, on our data.
However, we still need to find the optimal model within each of these classes,
by selecting the best hyperparameter (number of PCs for PC regression and $\lambda$
for lasso and ridge).
For that we will use
[*$k$-fold Cross Validation*](https://en.wikipedia.org/wiki/Cross-validation_(statistics))
on our training set.


## Tuning hyperparameters

The test set is only used to evaluate the *final* model.
To achieve this final model, we need to find the optimal hyperparameters,
i.e. the hyperparameters that best generalize the model to unseen data.
We can estimate this by using *k-fold cross validation* ($CV_k$) on
the training data.

The $CV_k$ estimates can be automatically computed for any
generalized linear model (generated with `glm()` and by extension `glmnet()`)
using the `cv.glm()` function from the
*[boot](https://CRAN.R-project.org/package=boot)* package.


# Example: PC regression evaluation

We start with the PC regression and look for the optimal number of PCs that minimizes
the MSE using $k$-fold Cross validation.
We then use this optimal number of PCs to train the final model and evaluate it
on the test data.


## k-fold Cross Validation to tune number of components

Conveniently, the `pcr` function from the `pls` package has an implementation for
k-fold Cross Validation. We simply need to set `validation = CV` and `segments = 20`
to perform 20-fold Cross Validation with PC regression.
If we don't specify `ncomp`, `pcr` will select the maximum number of PCs that can
be used for the CV.

Note that our training data `trainX` consists of 80 observations (rows).
If we perform 20-fold CV, that means we will split the data in 20 groups, so
each group will consist of 4 observations. At each CV cycle, one group will be left
out and the model will be trained on the remaining groups. This leaves us with
76 training observations for each CV cycle, so the maximal number of components
that can be used in the linear regression is 75.

```{r pcr-kCV}
## Set seed for reproducibility, kCV is a random process!
set.seed(123)

K <- 20

## The 'Y ~ .' notation means: fit Y by every other variable in the data
pcr_cv <- pcr(TRIM32 ~ ., data = train_data, validation = "CV", segments = K)
summary(pcr_cv)
```

We can plot the *root mean squared error of prediction* (RMSEP) for each number
of components as follows.^[Note: The solid black line indicates the cross-validated RMSEP,
while the dashed red line adjusts the RMSEP estimate downwards to account for the fact
that the PCR is trained on only $\frac{k-1}{k} n_{\text{train}} = 76$ instead of the full
$n_{\text{train}} = 80$ observations. See Equation (5) of
[*Mevik and Cederkvist, 2005*](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.887)
for the definition of the adjustment. If $k$ is relatively large, such as our $k = 20$,
the adjustment does not have a large impact and the solid and dashed lines are quite
similar.]

```{r pcr_cv-plot}
plot(pcr_cv, plottype = "validation")
```

The `pls` package also has a function `selectNcomp` to select the optimal number of components.
Here we use the "one-sigma" method, which returns the lowest number of components
for which the RMSE is within one standard error of the absolute minimum.
The function also allows plotting the result by specifying `plot = TRUE`.

```{r pcr-optimal-ncomp}
optimal_ncomp <- selectNcomp(pcr_cv, method = "onesigma", plot = TRUE)
```

The optimal number of components for our model is `r optimal_ncomp`.

```{r pcr-optimal-ncomp-print}
optimal_ncomp
```


## Validation on test data

We now use our optimal number of components to train the final PCR model.
This model is then validated on by generating predictions for the test data and
calculating the MSE.

We define a custom function to calculate the MSE.
Note that there is also an `MSEP` function in the `pls` package which does the
prediction and MSE calculation in one go.
But our own function will come in handy later for lasso and ridge regression.

```{r MSE}
# Mean Squared Error
## obs: observations; pred: predictions
MSE <- function(obs, pred){
  mean((drop(obs) - drop(pred))^2)
}
```

```{r final_pcr_model}
final_pcr_model <- pcr(TRIM32 ~ ., data = train_data, ncomp = optimal_ncomp)
pcr_preds <- predict(final_pcr_model, newdata = test_data, ncomp = optimal_ncomp)
(pcr_mse <- MSE(testY, pcr_preds))
```

This value on its own does not tell us very much, but we can use it to compare our
PCR model with other types of models later.

Finally, we plot the predicted values for our response variable (the TRIM32 gene expression)
against the actual observed values from our test set.

```{r pcr-predplot}
predplot(final_pcr_model, newdata = test_data, line = TRUE)
```



# Exercise: evaluate and compare prediction models

#### 1. Perform a lasso regression with 20-fold Cross Validation on the training data (`trainX`, `trainY`). Plot the results and select the optimal $\lambda$ parameter. Fit a final model with the selected $\lambda$ and validate it on the test data. {-}

*Hint*: use the `cv.glmnet()` function, for 20 folds CV, set `nfolds = 20` and
to use the MSE metric set `type.measure = "mse"`.
Go to `?cv.glmnet` for details.

<details><summary>Solution</summary>

```{r lasso-cv}
set.seed(123)
lasso_cv <- cv.glmnet(trainX, trainY, alpha = 1,
                      nfolds = K, type.measure = "mse")
lasso_cv
plot(lasso_cv)
```

Note that we can extract the fitted lasso regression object from the CV result
and make the coefficient profile plot as before.

```{r lasso-cv-coefficient-profile}
plot(lasso_cv$glmnet.fit, xvar = "lambda")
```

We can look for the $\lambda$ values that give the best result.
Here you have two possibilities :

1. `lambda.min`: the value of  $\lambda$ that gives the best result for the crossvalidation.
2. `lambda.1se`: the largest value of $\lambda$ such that the MSE is within 1 standard error
of the best result from the cross validation.

```{r}
lasso_cv$lambda.min
lasso_cv$lambda.1se
```

We will use `lambda.min` here to fit the final model and generate predictions on the test data.
Note that we don't actually have to redo the fitting, we can just use our existing
`lasso_cv` object, which already contains the fitted models for a range of `lambda` values.
We can use the `predict` function and specify the `s` argument (which confusingly sets `lambda` in this case)  to make predictions on the test data.

```{r}
lasso_preds <- predict(lasso_cv, s = lasso_cv$lambda.min, newx = testX)
## Calculate MSE
(lasso_mse <- MSE(testY, lasso_preds))
```
</details>


#### 2. Do the same for ridge regression. {-}

<details><summary>Solution</summary>

```{r ridge-cv}
set.seed(123)
ridge_cv <- cv.glmnet(trainX, trainY, alpha = 0,
                      nfolds = K, type.measure = "mse")
ridge_cv
plot(ridge_cv)
```

Since the MSE is minimized at the smallest considered $\lambda =$ `r ridge_cv$lambda.min`,
we should extend the grid to include smaller values than those that were chosen by the
default setting of `cv.glmnet()`. Intuitively, this is because the MSE might continue to
decrease beyond the left boundary of the plot.

```{r ridge-cv-alt}
set.seed(123)
ridge_cv <- cv.glmnet(trainX, trainY, alpha = 0,
                      nfolds = K, type.measure = "mse",
                      lambda = exp(seq(7, -2, by = -0.1)))
ridge_cv
plot(ridge_cv)
```

Note that we can extract the fitted ridge regression object from the CV result
and make the coefficient profile plot as before.

```{r ridge-cv-coefficient-profile}
plot(ridge_cv$glmnet.fit, xvar = "lambda")
```

We can look for the $\lambda$ values that give the best result.
Here you have two possibilities :

1. `lambda.min`: the value of  $\lambda$ that gives the best result for the crossvalidation.
2. `lambda.1se`: the largest value of $\lambda$ such that the MSE is within 1 standard error
of the best result from the cross validation.

```{r}
ridge_cv$lambda.min
ridge_cv$lambda.1se
```

We will use `lambda.min` here to fit the final model and generate predictions on the test data.
Note that we don't actually have to redo the fitting, we can just use our existing
`ridge_cv` object, which already contains the fitted models for a range of `lambda` values.
We can use the `predict` function and specify the `s` argument (which confusingly sets `lambda` in this case)  to make predictions on the test data.

```{r ridge-predictions}
ridge_preds <- predict(ridge_cv, s = ridge_cv$lambda.min, newx = testX)
## Calculate MSE
(ridge_mse <- MSE(testY, ridge_preds))
```

</details>


#### 3. Which of the models considered (PCR, lasso, ridge) performs best?. {-}

<details><summary>Solution</summary>

Based on the MSE, the ridge model performs best on the test data.

```{r, echo=FALSE}
knitr::kable(
  data.frame(
    "Model" = c("PCR", "Lasso", "Ridge"),
    "MSE" = c(pcr_mse, lasso_mse, ridge_mse)
  )
)
```
</details>


```{r, child="_session-info.Rmd"}
```
</div>
<div class="footer">
    <hr>
    This work is licensed under the <a href= "https://creativecommons.org/licenses/by-nc-sa/4.0">
    CC BY-NC-SA 4.0</a> licence.
</div>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeSourceEmbed("Lab3-Penalized-Regression.Rmd");
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>


</body>
</html>
